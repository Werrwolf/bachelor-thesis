{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shared tokens:37178\n",
      "unique roberta tokens:6278\n",
      "50265\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from transformers import RobertaTokenizer\n",
    "from tokenizers import Tokenizer, models, trainers, pre_tokenizers, decoders, processors\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import random\n",
    "\n",
    "data = pd.read_csv(\"labeled__dev_dataset.csv\")\n",
    "data.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "\n",
    "#### Step 1: Prep Work######################################################################################################################################################################################\n",
    "\n",
    "def extract_text_from_df(df):\n",
    "    \"\"\"\n",
    "    Extracts the 'log_entry' column from the DataFrame and returns a list of text entries.\n",
    "    \"\"\"\n",
    "    corpus = []\n",
    "    entries = df['log_entry'].tolist()\n",
    "    for entry in entries:\n",
    "        corpus.append(entry)\n",
    "    return corpus\n",
    "\n",
    "# \n",
    "def tokenize_text(corpus, remove_special_chars = True): \n",
    "    \"\"\"\n",
    "    Cleans the text by removing special characters.\n",
    "    If necessary expand by: |(\\/)   -- [Includes ' and /]\n",
    "    \"\"\"\n",
    "    domain_corpus = []\n",
    "    for text in corpus:\n",
    "        if remove_special_chars:\n",
    "            # set remove_special_chars to True if needed\n",
    "            text = re.sub(r'[(==+)|(\\|)|(\\')]', '', text)\n",
    "        domain_corpus.extend(text.split())\n",
    "    return domain_corpus\n",
    "\n",
    "corpus = extract_text_from_df(data)\n",
    "domain_corpus = tokenize_text(corpus)\n",
    "\n",
    "#### Step 2: Extract Tokens using BPE ######################################################################################################################################################################################\n",
    "\n",
    "# Initialize and train a custom BPE tokenizer\n",
    "tokenizer = Tokenizer(models.BPE())\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel()\n",
    "trainer = trainers.BpeTrainer(vocab_size=50265,min_frequency=4, special_tokens=[\"<s>\", \"<pad>\", \"</s>\", \"<unk>\", \"<mask>\"])\n",
    "tokenizer.train_from_iterator(domain_corpus, trainer)\n",
    "\n",
    "# Directory to save the tokenizer components\n",
    "save_dir = \"custom_tokenizer\"\n",
    "\n",
    "# Load the original RoBERTa tokenizer for vocab comparison\n",
    "roberta_tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "roberta_vocab = roberta_tokenizer.get_vocab()\n",
    "\n",
    "# Get the new vocabulary from the trained tokenizer\n",
    "log_vocab = tokenizer.get_vocab()\n",
    "\n",
    "#### Step 3: Merge vocabs ######################################################################################################################################################################################\n",
    "\n",
    "# vars: \n",
    "roBERTa_tokens = roberta_vocab.keys()\n",
    "log_tokens = log_vocab.keys()\n",
    "shared_tokens =  list(set(roberta_vocab.keys()).intersection(set(log_vocab.keys())))\n",
    "unique_roBERTa_tokens = list(roBERTa_tokens - shared_tokens)\n",
    "unique_log_tokens = list(log_tokens - shared_tokens)\n",
    "used_unique_roBERTa_tokens = []\n",
    "all_custom_tokens = []\n",
    "number_total_tokens = 50265\n",
    "all_custom_tokens = shared_tokens + unique_log_tokens + used_unique_roBERTa_tokens\n",
    "\n",
    "while len(all_custom_tokens) < number_total_tokens:\n",
    "    random_token = random.sample(unique_roBERTa_tokens, 1)[0]\n",
    "    used_unique_roBERTa_tokens.append(random_token)\n",
    "    unique_roBERTa_tokens.remove(random_token)\n",
    "    all_custom_tokens = shared_tokens + unique_log_tokens + used_unique_roBERTa_tokens\n",
    "\n",
    "print(f\"Shared tokens:{len(unique_log_tokens)}\")\n",
    "print(f\"unique roberta tokens:{len(used_unique_roBERTa_tokens)}\")\n",
    "print(len(unique_log_tokens) + len(used_unique_roBERTa_tokens) + len(shared_tokens)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Error while initializing BPE: Token `ou` out of vocabulary",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_4700\\1565815483.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBPE\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcustom_vocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmerges\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfiltered_merges\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpre_tokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpre_tokenizers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mByteLevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecoder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecoders\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mByteLevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mException\u001b[0m: Error while initializing BPE: Token `ou` out of vocabulary"
     ]
    }
   ],
   "source": [
    "#### Step 4: Assign Indices  ######################################################################################################################################################################################\n",
    "custom_vocab = {}\n",
    "\n",
    "# Assign original indices to used_roberta_tokens\n",
    "for token in shared_tokens:\n",
    "    custom_vocab[token] = roberta_vocab[token]\n",
    "\n",
    "for token in used_unique_roBERTa_tokens:\n",
    "     custom_vocab[token] = roberta_vocab[token]\n",
    "\n",
    "# Assign new indices to distinct tokens, ensuring no conflicts\n",
    "current_index = max(roberta_vocab.values()) + 1\n",
    "for token in unique_log_tokens:\n",
    "    custom_vocab[token] = current_index\n",
    "    current_index += 1\n",
    "\n",
    "# Save the custom vocabulary as a JSON file\n",
    "with open(os.path.join(save_dir, \"custom_vocab.json\"), \"w\", encoding=\"utf-8\") as vocab_file:\n",
    "    json.dump(custom_vocab, vocab_file, ensure_ascii=False)\n",
    "\n",
    "# Load the RoBERTa merges file and filter out invalid merges\n",
    "try:\n",
    "    with open(\"custom_tokenizer/roberta_base_merges.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "        merges = [tuple(line.split()) for line in f.read().split(\"\\n\")[:-1]]\n",
    "except IOError as e:\n",
    "    print(f\"Error loading RoBERTa merges file: {e}\")\n",
    "\n",
    "# Filter merges to only include those with tokens present in the custom vocabulary\n",
    "filtered_merges = [merge for merge in merges if merge[0] in custom_vocab and merge[1] in custom_vocab]\n",
    "\n",
    "# Check if all tokens in the merges are in the vocabulary\n",
    "missing_tokens = {merge[0] for merge in filtered_merges if merge[0] not in custom_vocab}\n",
    "missing_tokens.update({merge[1] for merge in filtered_merges if merge[1] not in custom_vocab})\n",
    "\n",
    "if missing_tokens:\n",
    "    print(f\"Warning: The following tokens are referenced in merges but are missing in the vocabulary: {missing_tokens}\")\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(models.BPE(vocab=custom_vocab, merges=filtered_merges))\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel()\n",
    "tokenizer.decoder = decoders.ByteLevel()\n",
    "tokenizer.post_processor = processors.ByteLevel(trim_offsets=True)\n",
    "\n",
    "\n",
    "# Step 5: Save final customized tokenizer  ######################################################################################################################################################################################\n",
    "try:\n",
    "    tokenizer.save(os.path.join(save_dir, \"custom_tokenizer.json\"))\n",
    "    print(f\"Custom tokenizer successfully saved as '{os.path.join(save_dir, 'custom_tokenizer.json')}'.\")\n",
    "except IOError as e:\n",
    "    print(f\"Error saving custom tokenizer: {e}\")\n",
    "\n",
    "# Save the merges file separately\n",
    "with open(os.path.join(save_dir, \"custom_merges.txt\"), \"w\", encoding= \"utf-8\") as merges_file:\n",
    "    for merge in filtered_merges:\n",
    "        merges_file.write(\" \".join(merge) + \"\\n\")\n",
    "\n",
    "# Save the tokenizer configuration separately\n",
    "tokenizer_config = {\n",
    "    \"do_lower_case\": False,\n",
    "    \"max_len\": 512,\n",
    "    \"vocab_size\": len(custom_vocab),\n",
    "    \"special_tokens_map_file\": None,\n",
    "    \"tokenizer_class\": \"RobertaTokenizer\",\n",
    "    \"model_max_length\": 512,\n",
    "    \"padding_side\": \"right\",\n",
    "    \"special_tokens\": {\n",
    "        \"<s>\": \"<s>\",\n",
    "        \"<pad>\": \"<pad>\",\n",
    "        \"</s>\": \"</s>\",\n",
    "        \"<unk>\": \"<unk>\",\n",
    "        \"<mask>\": \"<mask>\"\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(os.path.join(save_dir, \"tokenizer_config.json\"), \"w\", encoding=\"utf-8\") as config_file:\n",
    "    json.dump(tokenizer_config, config_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and use the custom tokenizer\n",
    "custom_tokenizer = Tokenizer.from_file(\"custom_tokenizer/custom_tokenizer.json\")\n",
    "\n",
    "# Example usage\n",
    "text_to_encode = \"well this is a success\"\n",
    "encoded = custom_tokenizer.encode(text_to_encode)\n",
    "print(encoded.tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom tokenizer (from https://arxiv.org/abs/2204.02685)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<font size=\"4\">\n",
    "For building the tokenizer, we employ a byte pair encoding (BPE) method to build a vocabulary of words and subwords from the cybersecurity corpora, as it is proven to have better performance versus word-based tokenizer. Character based encoding used in BPE allows for the learning of a small subword vocabulary that can encode any input text without introducing any \"unknown\" tokens. Our objective is to create a vocabulary that retains the tokens already provided in RoBERTa’s tokenizer while also incorporating additional unique cybersecurity related tokens. In this context, we extract 50, 265 tokens from the cybersecurity corpora to generate the initial token vocabulary ΨSec. We intentionally make the size of ΨSec the same with that of the RoBERTa’s token vocabulary ΨRoBERT a as we intended to imitate original RoBERTa’s design.\n",
    "If ΨSec represents the vocabulary set of SecureBERT, and ΨRoBERT a denotes the vocabulary set of original RoBERTa, both with size of 50, 265, ΨSec shares 32, 592 mutual tokens with ΨRoBERT a leaving 17, 673 tokens contribute uniquely to cybersecurity corpus, such as *firewall, breach, crack, ransomware, malware, phishing, mysql, kaspersky, obfuscated, and vulnerability*, where RoBERTa’s tokenizer analyzes those using byte pairs: \n",
    "<br>\n",
    "<br>\n",
    "Vmutual = ΨSec ∩ ΨRoBERT a → 32, 592 tokens <br>\n",
    "Vdistinct = ΨSec − ΨRoBERT a → 17, 673 tokens<br>\n",
    "<br>\n",
    "Studies shows utilizing complete words (not subwords) for those are common in specific domain, can enhance the performance during training since alignments may be more challenging to understand during model training, as target tokens often require attention from multiple source tokens. Hence, we choose all mutual terms and assign their original indices, while the remainder new tokens are assigned random indices with no conflict, where the original indices refers to the indices in RoBERTa’s tokenizer, to build our tokenizer. Ultimately, we develop a customized tokenizer with a vocabulary size similar to that of the original model, which includes tokens commonly seen in cybersecurity corpora in addition to cross-domain tokens. Our tokenizer encodes mutual tokens (Vmutual) as original model, ensuring that the model returns the appropriate pre-trained weights, while for new terms (Vdistinct) the indices and accordingly the weights would be random.\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overview over unique chars / amount of chars in words; pre-regex\n",
    "\n",
    "def word_count(words):\n",
    "    # Create a set 'word_set' to remove duplicate words from the input list.\n",
    "    word_set = set(words)\n",
    "    \n",
    "    # Create an empty dictionary 'word_counts' to store word counts.\n",
    "    word_counts = {}\n",
    "    \n",
    "    # Iterate over the unique words in 'word_set'.\n",
    "    for word in word_set:\n",
    "        # Count the occurrences of each word in the input list and store the count in 'word_counts'.\n",
    "        word_counts[word] = words.count(word)\n",
    "    \n",
    "    # Return the 'word_counts' dictionary.\n",
    "    return word_counts\n",
    "\n",
    "# Call the 'word_count' function with the 'words' list and print the word counts.\n",
    "#print(word_count(words)) \n",
    "uniques = set(\"\".join(words))\n",
    "print(uniques)\n",
    "\n",
    "for char in uniques:\n",
    "    amount = words.count(char)\n",
    "    # print(f\"Amount of { char} in words: {amount}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
