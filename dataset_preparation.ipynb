{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"labeled_dataset.csv\")\n",
    "data.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              count unique                                                top  \\\n",
      "task_id          22     17               16911dd9-d58b-df18-7329-000000000d5f   \n",
      "log_entry        22     17  @tu-cc-ci-adp-github-eof file:https://cc-ci.bm...   \n",
      "error_cluster    22      8                                              Bazel   \n",
      "error_type       22     11                            error executing command   \n",
      "\n",
      "              freq  \n",
      "task_id          2  \n",
      "log_entry        2  \n",
      "error_cluster    8  \n",
      "error_type       6  \n"
     ]
    }
   ],
   "source": [
    "print(data.describe(include=\"all\").T)\n",
    "\n",
    "for index, row in data['log_entry'].items():\n",
    "    log_entry = row\n",
    "    # Perform operations on the log_entry, do not actually print cause vsc will die\n",
    "    # print(log_entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFRobertaForMaskedLM.\n",
      "\n",
      "All the weights of TFRobertaForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForMaskedLM for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mutual tokens: 50265 \n",
      "Distinct tokens: 10\n",
      "Encoded text: [0, 133, 38956, 12333, 10, 801, 6999, 11, 5, 467, 4, 2]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from transformers import AutoTokenizer, TFAutoModelForMaskedLM\n",
    "from transformers import BasicTokenizer\n",
    "\n",
    "roberta_tokenizer = AutoTokenizer.from_pretrained(\"FacebookAI/roberta-base\")\n",
    "roberta_model = TFAutoModelForMaskedLM.from_pretrained(\"FacebookAI/roberta-base\", from_pt=True)\n",
    "\n",
    "# Extract tokens from cybersecurity corpus\n",
    "custom_tokens = ['firewall', 'breach', 'crack', 'ransomware', 'malware', 'phishing', 'mysql', 'kaspersky', 'obfuscated', 'vulnerability']\n",
    "\n",
    "# Create custom tokenizer\n",
    "log_roberta_vocab = roberta_tokenizer.get_vocab()\n",
    "# print(len(log_roberta_vocab))\n",
    "\n",
    "# add custom words to vocab\n",
    "log_roberta_vocab.update({token: len(log_roberta_vocab) for token in custom_tokens})\n",
    "# print(len(log_roberta_vocab))\n",
    "\n",
    "# # Analyze the vocabulary of the SecureBERT tokenizer\n",
    "original_tokens = set(log_roberta_vocab.keys()) & set(roberta_tokenizer.get_vocab().keys())\n",
    "distinct_tokens = set(log_roberta_vocab.keys()) - set(roberta_tokenizer.get_vocab().keys())\n",
    "\n",
    "# check original_tokens & distinct_tokens\n",
    "print(f\"Mutual tokens: {len(original_tokens)} \")\n",
    "print(f\"Distinct tokens: {len(distinct_tokens)}\")\n",
    "\n",
    "# # Use the tokenizer to encode text\n",
    "text = \"The firewall detected a potential breach in the system.\"\n",
    "encoded_text = roberta_tokenizer.encode(text)\n",
    "print(f\"Encoded text: {encoded_text}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
