{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from transformers import RobertaTokenizer\n",
    "from tokenizers import Tokenizer, models, trainers, pre_tokenizers, decoders, processors\n",
    "\n",
    "data = pd.read_csv(\"labeled_dataset.csv\")\n",
    "data.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "# data\n",
    "\n",
    "# print(data.describe(include=\"all\").T)\n",
    "\n",
    "for index, row in data['log_entry'].items():\n",
    "    log_entry = row\n",
    "    # Perform operations on the log_entry, do not actually print cause vsc will die\n",
    "    # print(log_entry)\n",
    "\n",
    "# data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Step 1: Prep Work\n",
    "\n",
    "# Function to extract text from the JSON content in the DataFrame\n",
    "# TODO: goal is having a list of all domain_corpus in the jsons, duplicates are needed. removing special chars tbd\n",
    "def extract_text_from_df(df):\n",
    "    corpus = []\n",
    "    entries = df['log_entry'].tolist()\n",
    "    for entry in entries:\n",
    "        corpus.append(entry)\n",
    "    return corpus\n",
    "\n",
    "\n",
    "# if necessary expand re by: |(\\/)   -- [Includes ' and /]\n",
    "def tokenize_text(corpus, remove_special_chars = False): \n",
    "    domain_corpus = []\n",
    "    for text in corpus:\n",
    "        if remove_special_chars:\n",
    "            # set remove_special_chars to True if needed\n",
    "            text = re.sub(r'[(==+)|(\\|)|(\\')]', '', text)\n",
    "        domain_corpus.extend(text.split())\n",
    "    return domain_corpus\n",
    "\n",
    "corpus = extract_text_from_df(data)\n",
    "# print(len(corpus))\n",
    "# print(corpus)\n",
    "domain_corpus = tokenize_text(corpus)\n",
    "#print(domain_corpus)\n",
    "print(len(domain_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Step 2: Extract Tokens using BPE\n",
    "\n",
    "# Load the original RoBERTa tokenizer\n",
    "roberta_tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "# Get the original vocabulary\n",
    "roberta_vocab = roberta_tokenizer.get_vocab()\n",
    "print(f\"ROBERTA Tokens: {len(roberta_vocab.keys())}\")\n",
    "\n",
    "# Initialize a BPE tokenizer\n",
    "tokenizer = Tokenizer(models.BPE())\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel()\n",
    "\n",
    "# Adjust the vocabulary size to match RoBERTa's\n",
    "trainer = trainers.BpeTrainer(vocab_size=50265,min_frequency=1, special_tokens=[\"<s>\", \"<pad>\", \"</s>\", \"<unk>\", \"<mask>\"])\n",
    "tokenizer.train_from_iterator(domain_corpus, trainer)\n",
    "\n",
    "# Get the new vocabulary from the trained tokenizer\n",
    "new_vocab = tokenizer.get_vocab()\n",
    "print(f\"New Tokens: {len(new_vocab.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Step 3: Merge Token List\n",
    "\n",
    "# Find mutual and distinct tokens\n",
    "mutual_tokens = set(roberta_vocab.keys()).intersection(set(new_vocab.keys()))\n",
    "distinct_tokens = set(new_vocab.keys()).difference(set(roberta_vocab.keys()))\n",
    "\n",
    "# Ensure the size of the new vocabulary matches the original\n",
    "assert len(mutual_tokens) + len(distinct_tokens) == 50265, \"Vocabulary size mismatch\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom tokenizer (from https://arxiv.org/abs/2204.02685)\n",
    "\n",
    "<font size=\"4\">\n",
    "For building the tokenizer, we employ a byte pair encoding (BPE) method to build a vocabulary of words and subwords from the cybersecurity corpora, as it is proven to have better performance versus word-based tokenizer. Character based encoding used in BPE allows for the learning of a small subword vocabulary that can encode any input text without introducing any \"unknown\" tokens. Our objective is to create a vocabulary that retains the tokens already provided in RoBERTa’s tokenizer while also incorporating additional unique cybersecurity related tokens. In this context, we extract 50, 265 tokens from the cybersecurity corpora to generate the initial token vocabulary ΨSec. We intentionally make the size of ΨSec the same with that of the RoBERTa’s token vocabulary ΨRoBERT a as we intended to imitate original RoBERTa’s design.\n",
    "If ΨSec represents the vocabulary set of SecureBERT, and ΨRoBERT a denotes the vocabulary set of original RoBERTa, both with size of 50, 265, ΨSec shares 32, 592 mutual tokens with ΨRoBERT a leaving 17, 673 tokens contribute uniquely to cybersecurity corpus, such as *firewall, breach, crack, ransomware, malware, phishing, mysql, kaspersky, obfuscated, and vulnerability*, where RoBERTa’s tokenizer analyzes those using byte pairs: \n",
    "<br>\n",
    "<br>\n",
    "Vmutual = ΨSec ∩ ΨRoBERT a → 32, 592 tokens <br>\n",
    "Vdistinct = ΨSec − ΨRoBERT a → 17, 673 tokens<br>\n",
    "<br>\n",
    "Studies shows utilizing complete words (not subwords) for those are common in specific domain, can enhance the performance during training since alignments may be more challenging to understand during model training, as target tokens often require attention from multiple source tokens. Hence, we choose all mutual terms and assign their original indices, while the remainder new tokens are assigned random indices with no conflict, where the original indices refers to the indices in RoBERTa’s tokenizer, to build our tokenizer. Ultimately, we develop a customized tokenizer with a vocabulary size similar to that of the original model, which includes tokens commonly seen in cybersecurity corpora in addition to cross-domain tokens. Our tokenizer encodes mutual tokens (Vmutual) as original model, ensuring that the model returns the appropriate pre-trained weights, while for new terms (Vdistinct) the indices and accordingly the weights would be random.\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GENEXT ANSWER\n",
    "\n",
    "To implement a custom tokenizer using Byte Pair Encoding (BPE) that incorporates both the original RoBERTa tokens and additional domain-specific tokens, follow these steps:\n",
    "\n",
    "1. **Prepare the Corpus**: Gather your domain-specific corpus.\n",
    "2. **Extract Tokens using BPE**: Use a BPE tokenizer to extract tokens from your corpus.\n",
    "3. **Merge Token Lists**: Combine the original RoBERTa tokens with the new domain-specific tokens.\n",
    "4. **Assign Indices**: Ensure mutual tokens retain their original indices, and assign new indices to the additional tokens.\n",
    "5. **Create the Custom Tokenizer**: Build the tokenizer using the merged vocabulary.\n",
    "\n",
    "### Step-by-Step Implementation\n",
    "\n",
    "#### Step 1: Prepare the Corpus\n",
    "Ensure you have your domain-specific text corpus ready for tokenization.\n",
    "\n",
    "```python\n",
    "# Example: Load your domain-specific corpus\n",
    "domain_corpus = [\"Your domain-specific text data goes here.\"]\n",
    "```\n",
    "\n",
    "#### Step 2: Extract Tokens using BPE\n",
    "Use the `tokenizers` library to perform BPE tokenization.\n",
    "\n",
    "```python\n",
    "from tokenizers import Tokenizer, models, trainers, pre_tokenizers, decoders, processors\n",
    "\n",
    "# Initialize a BPE tokenizer\n",
    "tokenizer = Tokenizer(models.BPE())\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel()\n",
    "\n",
    "# Train the tokenizer on your domain-specific corpus\n",
    "trainer = trainers.BpeTrainer(vocab_size=50265, special_tokens=[\"<s>\", \"<pad>\", \"</s>\", \"<unk>\", \"<mask>\"])\n",
    "tokenizer.train_from_iterator(domain_corpus, trainer)\n",
    "```\n",
    "\n",
    "#### Step 3: Merge Token Lists\n",
    "Load the original RoBERTa tokenizer and merge its tokens with the new ones.\n",
    "\n",
    "```python\n",
    "from transformers import RobertaTokenizer\n",
    "\n",
    "# Load the original RoBERTa tokenizer\n",
    "roberta_tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "# Get the original vocabulary\n",
    "roberta_vocab = roberta_tokenizer.get_vocab()\n",
    "\n",
    "# Get the new vocabulary from the trained tokenizer\n",
    "new_vocab = tokenizer.get_vocab()\n",
    "\n",
    "# Find mutual and distinct tokens\n",
    "mutual_tokens = set(roberta_vocab.keys()).intersection(set(new_vocab.keys()))\n",
    "distinct_tokens = set(new_vocab.keys()).difference(set(roberta_vocab.keys()))\n",
    "\n",
    "# Ensure the size of the new vocabulary matches the original\n",
    "assert len(mutual_tokens) + len(distinct_tokens) == 50265\n",
    "```\n",
    "\n",
    "#### Step 4: Assign Indices\n",
    "Assign indices to the mutual and distinct tokens.\n",
    "\n",
    "```python\n",
    "# Create a new vocabulary dictionary\n",
    "custom_vocab = {}\n",
    "\n",
    "# Assign original indices to mutual tokens\n",
    "for token in mutual_tokens:\n",
    "    custom_vocab[token] = roberta_vocab[token]\n",
    "\n",
    "# Assign new indices to distinct tokens, ensuring no conflicts\n",
    "current_index = max(roberta_vocab.values()) + 1\n",
    "for token in distinct_tokens:\n",
    "    custom_vocab[token] = current_index\n",
    "    current_index += 1\n",
    "```\n",
    "\n",
    "#### Step 5: Create the Custom Tokenizer\n",
    "Build the custom tokenizer using the merged vocabulary.\n",
    "\n",
    "```python\n",
    "# Create a new tokenizer with the custom vocabulary\n",
    "tokenizer = Tokenizer(models.BPE(custom_vocab))\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel()\n",
    "tokenizer.decoder = decoders.ByteLevel()\n",
    "tokenizer.post_processor = processors.ByteLevel(trim_offsets=True)\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer.save(\"custom_tokenizer.json\")\n",
    "\n",
    "# Load and use the custom tokenizer\n",
    "custom_tokenizer = Tokenizer.from_file(\"custom_tokenizer.json\")\n",
    "\n",
    "# Example usage\n",
    "encoded = custom_tokenizer.encode(\"Example text to tokenize.\")\n",
    "print(encoded.tokens)\n",
    "```\n",
    "\n",
    "### Explanation\n",
    "\n",
    "1. **Prepare the Corpus**: Load your domain-specific text data.\n",
    "2. **Extract Tokens using BPE**: Train a BPE tokenizer on your corpus to extract tokens.\n",
    "3. **Merge Token Lists**: Combine the RoBERTa tokens with the new tokens, ensuring mutual tokens are preserved.\n",
    "4. **Assign Indices**: Assign indices to mutual tokens as per the original RoBERTa tokenizer and assign new indices to distinct tokens.\n",
    "5. **Create the Custom Tokenizer**: Build and save the custom tokenizer with the merged vocabulary.\n",
    "\n",
    "This approach ensures that your custom tokenizer can handle both general and domain-specific tokens effectively, leveraging the pre-trained weights of RoBERTa for mutual tokens and incorporating new tokens for domain-specific terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage to encode text\n",
    "\n",
    "text = \"'/var/cache/bazel/execroot/ddad/application/foresight/activities/foresight_road_data/road_data_links_builder/src/map_api_links_converters.cpp', 'INFO:', '[pool-5-thread-3]', '/var/cache/bazel/execroot/ddad/application/foresight/activities/foresight_road_data/road_data_links_builder/src/map_api_links_utility.cpp', 'INFO:', '[pool-5-thread-12]', '/var/cache/bazel/execroot/ddad/application/foresight/activities/foresight_road_data/road_data_links_builder/src/map_api_links_warning_sign_converter.cpp', 'INFO:', '[pool-5-thread-15]', '/var/cache/bazel/execroot/ddad/application/foresight/activities/foresight_road_data/road_data_links_builder/src/map_api_node_data_list.cpp', 'INFO:', '[pool-5-thread-7]', '/var/cache/bazel/execroot/ddad/application/foresight/activities/foresight_road_data/road_data_links_builder/src/map_api_node_data_list_builder.cpp', 'INFO:', '[pool-5-thread-14]', '/var/cache/bazel/execroot/ddad/application/foresight/activities/foresight_road_data/road_data_links_builder/src/map_api_operator_equality.cpp', 'INFO:', '[pool-5-thread-5]', '/var/cache/bazel/execroot/ddad/application/foresight/activities/foresight_road_data/road_data_links_builder/src/pool/multiple_element_pool_hash_map.cpp', 'INFO:', '[pool-5-thread-8]', '/var/cache/bazel/execroot/ddad/application/foresight/activities/foresight_road_data/road_data_links_builder/src/pool/multiple_element_pool_vector.cpp', 'INFO:', '[pool-5-thread-5]', '/var/cache/bazel/execroot/ddad/application/foresight/activities/test/multi_component/most_probable_path_multi_component_test.cpp', 'INFO:', '[pool-5-thread-14]', \"\n",
    "\n",
    "encoded_text = roberta_tokenizer.encode(text)\n",
    "print(f\"Encoded text: {encoded_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overview over unique chars / amount of chars in words; pre-regex\n",
    "\n",
    "def word_count(words):\n",
    "    # Create a set 'word_set' to remove duplicate words from the input list.\n",
    "    word_set = set(words)\n",
    "    \n",
    "    # Create an empty dictionary 'word_counts' to store word counts.\n",
    "    word_counts = {}\n",
    "    \n",
    "    # Iterate over the unique words in 'word_set'.\n",
    "    for word in word_set:\n",
    "        # Count the occurrences of each word in the input list and store the count in 'word_counts'.\n",
    "        word_counts[word] = words.count(word)\n",
    "    \n",
    "    # Return the 'word_counts' dictionary.\n",
    "    return word_counts\n",
    "\n",
    "# Call the 'word_count' function with the 'words' list and print the word counts.\n",
    "#print(word_count(words)) \n",
    "uniques = set(\"\".join(words))\n",
    "print(uniques)\n",
    "\n",
    "for char in uniques:\n",
    "    amount = words.count(char)\n",
    "    # print(f\"Amount of { char} in words: {amount}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is how we can set the post-processing to give us the traditional BERT inputs: (https://huggingface.co/docs/tokenizers/quicktour)\n",
    "\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "tokenizer.post_processor = TemplateProcessing(\n",
    "    single=\"[CLS] $A [SEP]\",\n",
    "    pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
    "    special_tokens=[\n",
    "        (\"[CLS]\", tokenizer.token_to_id(\"[CLS]\")),\n",
    "        (\"[SEP]\", tokenizer.token_to_id(\"[SEP]\")),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have the following script to download json files from an aws bucket. I want to change it so it:\n",
    "1. checks all objects in the bucket\n",
    "2. chooses an predefined amount (e.g. 20) randomly\n",
    "3. saves them in a dir called \"logs\" using the same names as in thebucket\n",
    "4. if an object with the same name is already in the dir choose another one randomly until 20 new ones are downloaded"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
