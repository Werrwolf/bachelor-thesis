{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### first version\n",
    "\n",
    "\n",
    "import os\n",
    "import re\n",
    "\n",
    "def search_pattern_in_files(folder_path, pattern, start_pattern, end_pattern):\n",
    "   results = []\n",
    "   for root, dirs, files in os.walk(folder_path):\n",
    "       for file in files:\n",
    "           file_path = os.path.join(root, file)\n",
    "           with open(file_path, 'r') as f:\n",
    "               content = f.read()\n",
    "               matches = re.finditer(pattern, content)\n",
    "               if matches:\n",
    "                   for match in matches:\n",
    "                       start, end = match.start(), match.end()\n",
    "                       matched_section = content[start:end]\n",
    "                       start_point = re.findall(start_pattern, content[:start])\n",
    "                       end_point = re.findall(end_pattern, content[end:])\n",
    "                       if start_point and end_point:\n",
    "                           start_point = start_point[-1]\n",
    "                           end_point = end_point[0]\n",
    "                           before_text = content[content.rfind(start_point, 0, start):start]\n",
    "                           after_text = content[end:end + content.find(end_point, end)]\n",
    "                           result = {\n",
    "                               \"file_path\": file_path,\n",
    "                               \"start\": start,\n",
    "                               \"end\": end,\n",
    "                               \"matched_section\": matched_section,\n",
    "                               \"before_text\": before_text,\n",
    "                               \"after_text\": after_text\n",
    "                           }\n",
    "                           results.append(result)\n",
    "   return results\n",
    "\n",
    "def preprocess_log_files(folder_path, pattern, start_pattern, end_pattern):\n",
    "   results = search_pattern_in_files(folder_path, pattern, start_pattern, end_pattern)\n",
    "   file_count = 0\n",
    "   match_count = 0\n",
    "   for match in results:\n",
    "       file_path = match[\"file_path\"]\n",
    "       start = match[\"start\"]\n",
    "       end = match[\"end\"]\n",
    "       before_text = match[\"before_text\"]\n",
    "\n",
    "       new_file_path = os.path.join(\"preprocessed_logs\", f\"cropped_{os.path.basename(file_path)}_{start}-{end}\")\n",
    "       with open(new_file_path, 'w') as f:\n",
    "           f.write(before_text)\n",
    "\n",
    "       file_count += 1\n",
    "       match_count += 1\n",
    "\n",
    "   print(f\"Total matches found: {match_count}\")\n",
    "   print(f\"Total files created: {file_count}\")\n",
    "\n",
    "# Usage\n",
    "source_folder_path = \"logs\"\n",
    "pattern = r'\"failed\":\\s*true'\n",
    "start_pattern = r'\"branch\":\\s*\"master\",\\s*\"index\":'\n",
    "end_pattern = r'\"branch\":\\s*\"master\",\\s*\"index\":'\n",
    "\n",
    "preprocess_log_files(source_folder_path, pattern, start_pattern, end_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Simple script to gather info ###\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "def get_all_keys(data, prefix=''):\n",
    "    \"\"\"\n",
    "    Recursively collects all the keys in a JSON object.\n",
    "\n",
    "    :param data: The JSON data (already loaded into a Python object).\n",
    "    :param prefix: The prefix to append to the keys for nested dictionaries.\n",
    "    :return: A set of all keys in the JSON object.\n",
    "    \"\"\"\n",
    "    keys = set()\n",
    "\n",
    "    if isinstance(data, dict):\n",
    "        for k, v in data.items():\n",
    "            full_key = f\"{prefix}.{k}\" if prefix else k\n",
    "            keys.add(full_key)\n",
    "            keys.update(get_all_keys(v, full_key))\n",
    "    elif isinstance(data, list):\n",
    "        for item in data:\n",
    "            keys.update(get_all_keys(item, prefix))\n",
    "\n",
    "    return keys\n",
    "\n",
    "\n",
    "def get_keys_from_first_json_file_and_save(folder_path):\n",
    "    \"\"\"\n",
    "    Finds the first JSON file in the given folder, retrieves all keys, and saves them to a file.\n",
    "\n",
    "    :param folder_path: The path to the folder containing JSON files.\n",
    "    \"\"\"\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.json'):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            with open(file_path, 'r') as json_file:\n",
    "                data = json.load(json_file)\n",
    "                all_keys = get_all_keys(data)\n",
    "                keys_list = list(all_keys)\n",
    "                \n",
    "                # Save the keys to a file\n",
    "                with open('json_keys.txt', 'w') as keys_file:\n",
    "                    for key in keys_list:\n",
    "                        keys_file.write(f\"{key}\\n\")\n",
    "                print(f\"Keys saved to json_keys.txt\")\n",
    "                return  # Stop after processing the first JSON file\n",
    "\n",
    "    print(\"No JSON files found in the folder.\")\n",
    "\n",
    "# Usage example:\n",
    "# Replace 'source_folder_path' with the actual path to your folder containing JSON logs\n",
    "source_folder_path = 'logs'\n",
    "all_keys = get_keys_from_first_json_file_and_save(source_folder_path)\n",
    "print(all_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ONe working Version, DO NOT TOUCH\n",
    "\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "def clear_directory(directory_path):\n",
    "    \"\"\"\n",
    "    Clears all the contents of the given directory.\n",
    "\n",
    "    :param directory_path: The path to the directory to clear.\n",
    "    \"\"\"\n",
    "    for filename in os.listdir(directory_path):\n",
    "        file_path = os.path.join(directory_path, filename)\n",
    "        try:\n",
    "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                os.unlink(file_path)\n",
    "            elif os.path.isdir(file_path):\n",
    "                shutil.rmtree(file_path)\n",
    "        except Exception as e:\n",
    "            print(f'Failed to delete {file_path}. Reason: {e}')\n",
    "\n",
    "def extract_error_info_from_file(json_file_path):\n",
    "    \"\"\"\n",
    "    Reads a JSON file and extracts error information from the \"plays\" section, including all tasks where \"failed\" is true,\n",
    "    and retrieves \"stderr\" and \"stdout_lines\" for the host's node.\n",
    "\n",
    "    :param json_file_path: The file path to the JSON data file.\n",
    "    :return: A list of error information dictionaries.\n",
    "    \"\"\"\n",
    "    error_info_list = []\n",
    "\n",
    "    try:\n",
    "        with open(json_file_path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "\n",
    "        # Check if the JSON data is a list\n",
    "        if not isinstance(data, list):\n",
    "            raise ValueError(\"JSON data must be a list.\")\n",
    "\n",
    "        # Iterate over each element in the list to find dictionaries with a 'plays' key\n",
    "        for item in data:\n",
    "            if isinstance(item, dict) and 'plays' in item:\n",
    "                # Iterate over each play in the \"plays\" list\n",
    "                for play in item['plays']:\n",
    "                    # Assuming each play is a dictionary and contains a 'tasks' key\n",
    "                    if 'tasks' in play:\n",
    "                        for task in play['tasks']:\n",
    "                            # Assuming each task is a dictionary and contains a 'hosts' key\n",
    "                            if 'hosts' in task:\n",
    "                                for node, host_info in task['hosts'].items():\n",
    "                                    # Check if the task failed for the current host's node\n",
    "                                    if host_info.get('failed'):\n",
    "                                        error_info = {\n",
    "                                            'task_name': task.get('name', 'Unnamed task'),\n",
    "                                            'node': node,\n",
    "                                            'stderr': host_info.get('stderr', ''),\n",
    "                                            'stdout_lines': host_info.get('stdout_lines', [])\n",
    "                                        }\n",
    "                                        error_info_list.append(error_info)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"An error occurred while decoding JSON from {json_file_path}: {e}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"The file {json_file_path} was not found.\")\n",
    "    except IOError as e:\n",
    "        print(f\"An I/O error occurred while handling {json_file_path}: {e}\")\n",
    "    except ValueError as e:\n",
    "        print(f\"An error occurred while processing {json_file_path}: {e}\")\n",
    "\n",
    "    return error_info_list\n",
    "\n",
    "def extract_and_save_error_info(directory_path, output_directory):\n",
    "    \"\"\"\n",
    "    Extracts error information from all JSON files in the given directory and saves it to new files in the output directory.\n",
    "\n",
    "    :param directory_path: The path to the directory containing JSON files.\n",
    "    :param output_directory: The path to the directory where the error information files will be saved.\n",
    "    \"\"\"\n",
    "    # Check if the output directory exists, create it if it doesn't, or clear it if it does\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "    else:\n",
    "        clear_directory(output_directory)\n",
    "\n",
    "    # Loop through all files in the directory\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith('.json'):\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            try:\n",
    "                # Extract error info from each JSON file\n",
    "                error_info = extract_error_info_from_file(file_path)\n",
    "                if error_info:  # If there is error info, save it to a new file\n",
    "                    new_filename = os.path.splitext(filename)[0] + '_cropped.json'\n",
    "                    new_file_path = os.path.join(output_directory, new_filename)\n",
    "                    with open(new_file_path, 'w') as new_file:\n",
    "                        json.dump(error_info, new_file, indent=4)\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred while processing {filename}: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "logs_directory_path = 'logs'\n",
    "output_directory_path = 'preprocessed_logs'\n",
    "\n",
    "extract_and_save_error_info(logs_directory_path, output_directory_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working pattern matching \n",
    "\n",
    "# Function to compile the patterns from error_patterns.py\n",
    "import json\n",
    "import error_patterns\n",
    "import re\n",
    "import os\n",
    "INFRA_PATTERNS = error_patterns.INFRA_PATTERNS\n",
    "BUILD_PATTERNS = error_patterns.BUILD_PATTERNS\n",
    "\n",
    "def compile_patterns(patterns_dict):\n",
    "    compiled_patterns = {}\n",
    "    for main_category, patterns in patterns_dict.items():\n",
    "        if isinstance(patterns, dict):\n",
    "            for sub_category, sub_patterns in patterns.items():\n",
    "                for pattern in sub_patterns:\n",
    "                    compiled_patterns.setdefault((main_category, sub_category), []).append(re.compile(pattern))\n",
    "        else:\n",
    "            for pattern in patterns:\n",
    "                compiled_patterns.setdefault((main_category, \"\"), []).append(re.compile(pattern))\n",
    "    return compiled_patterns\n",
    "\n",
    "# Compile the patterns\n",
    "compiled_infra_patterns = compile_patterns(INFRA_PATTERNS)\n",
    "compiled_build_patterns = compile_patterns(BUILD_PATTERNS)\n",
    "\n",
    "# print(compiled_infra_patterns)\n",
    "# print(compiled_build_patterns)\n",
    "\n",
    "#### Working Code, Type + subtype, INFRA_PATTERN, file: ef7ce17ae108085e8a26e1f2046e1cf73e8c976_cropped.json\n",
    "\n",
    "# Function to check a log entry against compiled patterns\n",
    "def check_log_entry(log_entry, compiled_patterns):\n",
    "    matches = []\n",
    "    for (main_category, sub_category), regex_list in compiled_patterns.items():\n",
    "        for regex in regex_list:\n",
    "            if regex.search(log_entry):\n",
    "                # If a subcategory exists, include it in the output\n",
    "                category = f\"{main_category} - {sub_category}\" if sub_category else main_category\n",
    "                # Append a tuple with the category, subcategory, and the matching pattern\n",
    "                matches.append((category, sub_category, regex.pattern))\n",
    "    return matches\n",
    "\n",
    "# Function to process a log file\n",
    "def process_log_file(file_path, compiled_patterns):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        log_entries = json.load(file)\n",
    "        \n",
    "        # Iterate over the log entries\n",
    "        for entry in log_entries:\n",
    "            # Concatenate the stdout_lines if present into a single string\n",
    "            stdout_text = \"\\n\".join(entry['stdout_lines']) if 'stdout_lines' in entry else \"\"\n",
    "            # Check the concatenated stdout_lines against the compiled regex patterns\n",
    "            matches = check_log_entry(stdout_text, compiled_patterns)\n",
    "            if matches:\n",
    "                for match in matches:\n",
    "                    main_category, sub_category, pattern = match\n",
    "                    print(f\"Main Category: {main_category}, Subcategory: {sub_category}, Pattern: {pattern}\")\n",
    "            else:\n",
    "                print(\"No matches found\")\n",
    "\n",
    "#USAGE:\n",
    "path_example_logfile = 'preprocessed_logs/882dafcc748b4f0695ce486241a05ad2_cropped.json'\n",
    "\n",
    "# Assuming compiled_infra_patterns and compiled_build_patterns are already defined\n",
    "# Combine all compiled patterns into a single dictionary before processing the log file\n",
    "all_compiled_patterns = {**compiled_infra_patterns, **compiled_build_patterns}\n",
    "process_log_file(path_example_logfile, all_compiled_patterns)\n",
    "\n",
    "\n",
    "#######################################################################\n",
    "#### For INFRA_PATTERN, file: ef7ce17ae108085e8a26e1f2046e1cf73e8c976_cropped.json\n",
    "\n",
    "def process_log_file(file_path, compiled_patterns):\n",
    "    with open(file_path, 'r') as file:\n",
    "        log_entries = json.load(file)\n",
    "        \n",
    "        # Iterate over the log entries\n",
    "        for entry in log_entries:\n",
    "            # Concatenate the stdout_lines if present into a single string\n",
    "            stdout_text = \"\\n\".join(entry['stdout_lines']) if 'stdout_lines' in entry else \"\"\n",
    "            # Check the concatenated stdout_lines against the compiled regex patterns\n",
    "            matches = check_log_entry(stdout_text, compiled_patterns)\n",
    "            if matches:\n",
    "                for match in matches:\n",
    "                    main_category, sub_category, pattern = match\n",
    "                    print(f\"Main Category: {main_category}, Subcategory: {sub_category}, Pattern: {pattern}\")\n",
    "            else:\n",
    "                print(\"No matches found\")\n",
    "\n",
    "\n",
    "# Function to check a log entry against compiled patterns\n",
    "def check_log_entry(log_entry, compiled_patterns):\n",
    "    matches = []\n",
    "    for (main_category, sub_category), regex_list in compiled_patterns.items():\n",
    "        for regex in regex_list:\n",
    "            if regex.search(log_entry):\n",
    "                # If a subcategory exists, include it in the output\n",
    "                category = f\"{main_category} - {sub_category}\" if sub_category else main_category\n",
    "                # Append a tuple with the category, subcategory, and the matching pattern\n",
    "                matches.append((category, sub_category, regex.pattern))\n",
    "    return matches\n",
    "\n",
    "#USAGE:\n",
    "path_example_logfile = 'preprocessed_logs/9ef7ce17ae108085e8a26e1f2046e1cf73e8c976_cropped.json'\n",
    "\n",
    "process_log_file(path_example_logfile, compiled_infra_patterns)\n",
    "\n",
    "#######################################################################\n",
    "#### same for build patterns\n",
    "\n",
    "def process_log_file(file_path, compiled_patterns):\n",
    "    with open(file_path, 'r') as file:\n",
    "        log_entries = json.load(file)\n",
    "        \n",
    "        # Iterate over the log entries\n",
    "        for entry in log_entries:\n",
    "            # Concatenate the stdout_lines if present into a single string\n",
    "            stdout_text = \"\\n\".join(entry['stdout_lines']) if 'stdout_lines' in entry else \"\"\n",
    "            # Check the concatenated stdout_lines against the compiled regex patterns\n",
    "            matches = check_log_entry(stdout_text, compiled_patterns)\n",
    "            if matches:\n",
    "                for match in matches:\n",
    "                    main_category, sub_category, pattern = match\n",
    "                    print(f\"Main Category: {main_category}, Subcategory: {sub_category}, Pattern: {pattern}\")\n",
    "            else:\n",
    "                print(\"No matches found\")\n",
    "\n",
    "\n",
    "# Function to check a log entry against compiled patterns\n",
    "def check_log_entry(log_entry, compiled_patterns):\n",
    "    matches = []\n",
    "    for (main_category, sub_category), regex_list in compiled_patterns.items():\n",
    "        for regex in regex_list:\n",
    "            if regex.search(log_entry):\n",
    "                # If a subcategory exists, include it in the output\n",
    "                category = f\"{main_category} - {sub_category}\" if sub_category else main_category\n",
    "                # Append a tuple with the category, subcategory, and the matching pattern\n",
    "                matches.append((category, sub_category, regex.pattern))\n",
    "    return matches\n",
    "\n",
    "#USAGE:\n",
    "path_example_logfile = 'preprocessed_logs/9ef7ce17ae108085e8a26e1f2046e1cf73e8c976_cropped.json'\n",
    "\n",
    "process_log_file(path_example_logfile, compiled_build_patterns)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
