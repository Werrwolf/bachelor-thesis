{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "import warnings\n",
    "import random\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel, AdamW, get_cosine_schedule_with_warmup\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchmetrics.functional.classification import auroc\n",
    "from transformers import logging as transformers_logging\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, ReduceLROnPlateau\n",
    "\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "transformers_logging.set_verbosity_error()\n",
    "warnings.filterwarnings(\n",
    "    'ignore',\n",
    "    message=\"The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck.\"\n",
    ")\n",
    "\n",
    "train_data = pd.read_csv(\"train.csv\")\n",
    "train_path = \"train.csv\"\n",
    "\n",
    "val_data = pd.read_csv(\"val.csv\")\n",
    "val_path = \"val.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Training, but w/o lightning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a new column, switching around healthy  & unhealthy ( only for exploration purposes, can be removed!)\n",
    "train_data['unhealthy'] = np.where(train_data['healthy'] == 1, 0, 1)\n",
    "\n",
    "attributes = ['antagonize', 'condescending', 'dismissive', \"generalisation\", \"generalisation_unfair\", \"hostile\", \"unhealthy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UCC_Dataset(Dataset):\n",
    "    \"\"\"\n",
    "    @param sample: Because it is a highly unbalanced dataset, using the sample size manually cuts of the majority class once the max is reached\n",
    "    \"\"\"\n",
    "    def __init__(self, data_path, tokenizer, attributes, max_token_length: int = 64, sample = 5000):\n",
    "        self.data_path = data_path\n",
    "        self.tokenizer = tokenizer\n",
    "        self.attributes = attributes\n",
    "        self.max_token_length = max_token_length\n",
    "        self.sample = sample\n",
    "        self.__prepare_data()\n",
    "\n",
    "    ## ( __ makes it a private funct)\n",
    "    def __prepare_data(self):\n",
    "        data = pd.read_csv(self.data_path)\n",
    "        data['unhealthy'] = np.where(data['healthy'] == 1, 0, 1)\n",
    "\n",
    "        if self.sample is not None:\n",
    "            # if there is a positive attribute\n",
    "            unhealthy = data.loc[data[attributes].sum(axis=1) > 0 ]\n",
    "            healthy = data.loc[data[attributes].sum(axis=1) == 0 ]\n",
    "            # random state for reproduction\n",
    "            self.data = pd.concat([unhealthy, healthy.sample(self.sample, random_state=666)])\n",
    "\n",
    "        else:\n",
    "            self.data = data\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return(len(self.data))\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        item = self.data.iloc[index]\n",
    "        comment = str(item.comment)\n",
    "\n",
    "        # turn things into a tensor\n",
    "        attributes = torch.FloatTensor(item[self.attributes])\n",
    "        # takes input as string, the \"special tokens\" ensures formatting by adding beggining & end of sentence tokens\n",
    "        # Truncate: comments to max_length\n",
    "        # padding: pad tokens < 512 to may_length\n",
    "        # return_atention-mask: returns 1 everywhere except for paddings (0)\n",
    "        tokens = self.tokenizer.encode_plus (comment,\n",
    "                                                add_special_tokens = True,\n",
    "                                                return_tensors=\"pt\", \n",
    "                                                truncation =True,\n",
    "                                                max_length = self.max_token_length,\n",
    "                                                padding = \"max_length\",\n",
    "                                                return_attention_mask=True)\n",
    "        \n",
    "        return {'input_ids': tokens.input_ids.flatten(), 'attention_mask': tokens.attention_mask.flatten(), 'labels': attributes }\n",
    "\n",
    "model_name = \"roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "ucc_ds = UCC_Dataset(train_path, tokenizer, attributes)\n",
    "\n",
    "ucc_ds_val = UCC_Dataset(val_path, tokenizer, attributes, sample =None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UCC_DataModule:\n",
    "    def __init__(self, train_path, val_path, attributes, batch_size: int = 16, max_token_len: int = 64, model_name=\"roberta-base\"):\n",
    "        self.train_path = train_path\n",
    "        self.val_path = val_path\n",
    "        self.attributes = attributes\n",
    "        self.batch_size = batch_size\n",
    "        self.max_token_len = max_token_len\n",
    "        self.model_name = model_name\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "        self.train_dataset = None\n",
    "        self.val_dataset = None\n",
    "\n",
    "    def setup(self):\n",
    "        self.train_dataset = UCC_Dataset(self.train_path, self.tokenizer, self.attributes, max_token_length=self.max_token_len)\n",
    "        self.val_dataset = UCC_Dataset(self.val_path, self.tokenizer, self.attributes, max_token_length=self.max_token_len, sample=None)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, num_workers=4, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, num_workers=4, shuffle=False)\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, num_workers=4, shuffle=False)\n",
    "\n",
    "ucc_data_module = UCC_DataModule(train_path, val_path, attributes)\n",
    "# create datasets\n",
    "ucc_data_module.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UCC_Classifier(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.pretrained_model = AutoModel.from_pretrained(config[\"model_name\"], return_dict=True)\n",
    "        \n",
    "        # appending a classification layer (=\"head\") & a hidden layer before final_layer\n",
    "        self.hidden = nn.Linear(self.pretrained_model.config.hidden_size, self.pretrained_model.config.hidden_size)\n",
    "        self.classifier = nn.Linear(self.pretrained_model.config.hidden_size, self.config[\"n_labels\"])\n",
    "        \n",
    "        # initializing custom layers, optional, would also be done automatically, but better performance if stated explicitly)\n",
    "        torch.nn.init.xavier_uniform_(self.hidden.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.classifier.weight)\n",
    "\n",
    "         # define loss function\n",
    "        self.loss_func = nn.BCEWithLogitsLoss(reduction=\"mean\")\n",
    "        \n",
    "        # dropout layer (ensures no overfitting); randomly turns on/off random nodes for each training loop; so  model is not dependant on specific node\n",
    "        self.dropout = nn.Dropout()\n",
    "\n",
    "    # Forward pass, labels only needed while training, not for prediction, so set to none by default\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        # RoBERTa model output\n",
    "        model_output = self.pretrained_model(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask\n",
    "        )\n",
    "\n",
    "        # output is given for every single token (512 per batch for me), so: take mean from all\n",
    "        # there is the option to usepooled Output, which uses the first and last few tokens, depends on usecase\n",
    "        # Access the last hidden state\n",
    "        last_hidden_state = model_output.last_hidden_state\n",
    "\n",
    "        # Take the mean of the last hidden state\n",
    "        pooled_output = torch.mean(last_hidden_state, dim=1)\n",
    "        \n",
    "        # Print the output to inspect the structure\n",
    "        # print(\"model_output type:\", type(model_output))\n",
    "        # print(\"model_output keys:\", model_output.keys())\n",
    "\n",
    "        # # Access the relevant output based on the structure\n",
    "        # if \"last_hidden_state\" in model_output:\n",
    "        #     output = model_output.last_hidden_state\n",
    "        #     print(\"last_hidden_state shape:\", output.shape)\n",
    "        # elif \"pooler_output\" in model_output:\n",
    "        #     output = model_output.pooler_output\n",
    "        #     print(\"pooler_output shape:\", output.shape)\n",
    "        # else:\n",
    "        #     raise ValueError(\"Unexpected output format from the pretrained model\")\n",
    "\n",
    "\n",
    "        # NN-layers\n",
    "        pooled_output = self.hidden(pooled_output)  # pass sentence(or however tokens are pooled) through hidden layer\n",
    "        pooled_output = self.dropout(pooled_output) # pass through dropout layer\n",
    "        pooled_output = F.relu(pooled_output)   # pass through activatioin function( relu; in this case)\n",
    "        logits = self.classifier(pooled_output)     # pass through classification layer\n",
    "\n",
    "        # calculate loss\n",
    "        loss = 0\n",
    "        if labels is not None:\n",
    "            loss = self.loss_func(logits.view(-1, self.config[\"n_labels\"]), labels.view(-1, self.config[\"n_labels\"]))\n",
    "\n",
    "        # logits = fancy name for model output\n",
    "        return loss, logits\n",
    "\n",
    "    def training_step(self, batch, batch_index):\n",
    "        # get & unpack the output dict ( as in.  ucc_ds.__getitem__(0) into self)\n",
    "        loss, logits = self(**batch)\n",
    "        # save training loss\n",
    "        self.log(\"train_loss\", loss, prog_bar=True, logger= True)\n",
    "        return {\"loss\": loss, \"predictions\": logits, \"labels\": batch[\"labels\"]}\n",
    "\n",
    "\n",
    "    def validation_step(self, batch, batch_index):\n",
    "        loss, logits = self(**batch)\n",
    "        self.log(\"validation_loss\", loss, prog_bar=True, logger= True)\n",
    "        return {\"val_loss\": loss, \"predictions\": logits, \"labels\": batch[\"labels\"]}\n",
    "\n",
    "\n",
    "    def prediction_step(self, batch, batch_index):\n",
    "        __, logits = self(**batch)\n",
    "        # prediction:\n",
    "        return logits\n",
    "\n",
    "    # cofigure optimizers\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = AdamW(self.parameters(), lr=self.config[\"lr\"], weight_decay=self.config[\"w_decay\"])\n",
    "        # Scheduler: \n",
    "        total_steps = self.config[\"train_size\"] / self.config[\"bs\"]\n",
    "        # warmup in config is a percentage\n",
    "        warmup_steps = math.floor(total_steps * self.config[\"warmup\"])\n",
    "\n",
    "        scheduler = get_cosine_schedule_with_warmup(optimizer, warmup_steps, total_steps)\n",
    "        return [optimizer], [scheduler] \n",
    "    \n",
    "\n",
    "\n",
    "# Training loop\n",
    "def train(model, ucc_data_module, config):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # Set up optimizer and scheduler\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=config[\"lr\"], weight_decay=config[\"w_decay\"])\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=config[\"n_epochs\"], eta_min=1e-6)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(config[\"n_epochs\"]):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for batch in ucc_data_module.train_dataloader():\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            optimizer.zero_grad()\n",
    "            loss, _ = model(**batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch in ucc_data_module.val_dataloader():\n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                loss, _ = model(**batch)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{config['n_epochs']}], Train Loss: {train_loss/len(ucc_data_module.train_dataloader()):.4f}, Val Loss: {val_loss/len(ucc_data_module.val_dataloader()):.4f}\")\n",
    "        scheduler.step()\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Usage\n",
    "config = {\n",
    "    'model_name': \"distilroberta-base\",\n",
    "    \"n_labels\": len(attributes), \n",
    "    \"bs\": 64,              # batch size \n",
    "    \"lr\": 1.5e-6,\n",
    "    \"warmup\": 0.2,\n",
    "    \"train_size\": len(ucc_data_module.train_dataloader()), \n",
    "    \"w_decay\": 0.001,\n",
    "    \"n_epochs\": 1\n",
    "    }\n",
    "\n",
    "train_dataset = UCC_Dataset(train_path, tokenizer, attributes)\n",
    "val_dataset = UCC_Dataset(val_path, tokenizer, attributes, sample=None)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=config['bs'], shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=config['bs'], shuffle=False)\n",
    "\n",
    "model = UCC_Classifier(config)\n",
    "trained_model = train(model, ucc_data_module, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict / Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def predict(model, ucc_data_module):\n",
    "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#     model.to(device)\n",
    "#     model.eval()\n",
    "\n",
    "#     predictions = []\n",
    "#     with torch.no_grad():\n",
    "#         for batch in ucc_data_module.test_dataloader():\n",
    "#             batch = {k: v.to(device) for k, v in batch.items()}\n",
    "#             _, logits = model(**batch)\n",
    "#             preds = torch.sigmoid(logits).squeeze().cpu().numpy()\n",
    "#             predictions.extend(preds)\n",
    "\n",
    "#     return predictions\n",
    "\n",
    "def predict_single(model, ucc_data_module):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Get a random sample from the test set\n",
    "    test_dataloader = ucc_data_module.test_dataloader()\n",
    "    random_index = random.randint(0, len(test_dataloader) - 1)\n",
    "    batch = next(iter(test_dataloader))\n",
    "    for k, v in batch.items():\n",
    "        batch[k] = v.unsqueeze(0).to(device)\n",
    "\n",
    "    # Make a prediction for the single sample\n",
    "    with torch.no_grad():\n",
    "        loss, logits = model(**batch)\n",
    "        prediction = torch.sigmoid(logits).squeeze().item()\n",
    "\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict all from test set\n",
    "# predictions = predict(trained_model, ucc_data_module)\n",
    "\n",
    "# predict random from test set\n",
    "single_prediction = predict_single(trained_model, ucc_data_module)\n",
    "print(f\"Predicted probability: {single_prediction:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ALL IN ONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################### ALL IN ONE COPY + PASTE VERSION ##################################################\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "import warnings\n",
    "import random\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel, AdamW, get_cosine_schedule_with_warmup\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchmetrics.functional.classification import auroc\n",
    "from transformers import logging as transformers_logging\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, ReduceLROnPlateau\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "transformers_logging.set_verbosity_error()\n",
    "warnings.filterwarnings(\n",
    "    'ignore',\n",
    "    message=\"The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck.\"\n",
    ")\n",
    "\n",
    "train_data = pd.read_csv(\"train.csv\")\n",
    "train_path = \"train.csv\"\n",
    "\n",
    "val_data = pd.read_csv(\"val.csv\")\n",
    "val_path = \"val.csv\"\n",
    "\n",
    "# creating a new column, switching around healthy  & unhealthy ( only for exploration purposes, can be removed!)\n",
    "train_data['unhealthy'] = np.where(train_data['healthy'] == 1, 0, 1)\n",
    "\n",
    "attributes = ['antagonize', 'condescending', 'dismissive', \"generalisation\", \"generalisation_unfair\", \"hostile\", \"unhealthy\"]\n",
    "\n",
    "class UCC_Dataset(Dataset):\n",
    "    \"\"\"\n",
    "    @param sample: Because it is a highly unbalanced dataset, using the sample size manually cuts of the majority class once the max is reached\n",
    "    \"\"\"\n",
    "    def __init__(self, data_path, tokenizer, attributes, max_token_length: int = 64, sample = 5000):\n",
    "        self.data_path = data_path\n",
    "        self.tokenizer = tokenizer\n",
    "        self.attributes = attributes\n",
    "        self.max_token_length = max_token_length\n",
    "        self.sample = sample\n",
    "        self.__prepare_data()\n",
    "\n",
    "    ## ( __ makes it a private funct)\n",
    "    def __prepare_data(self):\n",
    "        data = pd.read_csv(self.data_path)\n",
    "        data['unhealthy'] = np.where(data['healthy'] == 1, 0, 1)\n",
    "\n",
    "        if self.sample is not None:\n",
    "            # if there is a positive attribute\n",
    "            unhealthy = data.loc[data[attributes].sum(axis=1) > 0 ]\n",
    "            healthy = data.loc[data[attributes].sum(axis=1) == 0 ]\n",
    "            # random state for reproduction\n",
    "            self.data = pd.concat([unhealthy, healthy.sample(self.sample, random_state=666)])\n",
    "\n",
    "        else:\n",
    "            self.data = data\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return(len(self.data))\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        item = self.data.iloc[index]\n",
    "        comment = str(item.comment)\n",
    "\n",
    "        # turn things into a tensor\n",
    "        attributes = torch.FloatTensor(item[self.attributes])\n",
    "        # takes input as string, the \"special tokens\" ensures formatting by adding beggining & end of sentence tokens\n",
    "        # Truncate: comments to max_length\n",
    "        # padding: pad tokens < 512 to may_length\n",
    "        # return_atention-mask: returns 1 everywhere except for paddings (0)\n",
    "        tokens = self.tokenizer.encode_plus (comment,\n",
    "                                                add_special_tokens = True,\n",
    "                                                return_tensors=\"pt\", \n",
    "                                                truncation =True,\n",
    "                                                max_length = self.max_token_length,\n",
    "                                                padding = \"max_length\",\n",
    "                                                return_attention_mask=True)\n",
    "        \n",
    "        return {'input_ids': tokens.input_ids.flatten(), 'attention_mask': tokens.attention_mask.flatten(), 'labels': attributes }\n",
    "\n",
    "model_name = \"roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "ucc_ds = UCC_Dataset(train_path, tokenizer, attributes)\n",
    "\n",
    "ucc_ds_val = UCC_Dataset(val_path, tokenizer, attributes, sample =None)\n",
    "\n",
    "\n",
    "class UCC_DataModule:\n",
    "    def __init__(self, train_path, val_path, attributes, batch_size: int = 16, max_token_len: int = 64, model_name=\"roberta-base\"):\n",
    "        self.train_path = train_path\n",
    "        self.val_path = val_path\n",
    "        self.attributes = attributes\n",
    "        self.batch_size = batch_size\n",
    "        self.max_token_len = max_token_len\n",
    "        self.model_name = model_name\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "        self.train_dataset = None\n",
    "        self.val_dataset = None\n",
    "\n",
    "    def setup(self):\n",
    "        self.train_dataset = UCC_Dataset(self.train_path, self.tokenizer, self.attributes, max_token_length=self.max_token_len)\n",
    "        self.val_dataset = UCC_Dataset(self.val_path, self.tokenizer, self.attributes, max_token_length=self.max_token_len, sample=None)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, num_workers=0, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, num_workers=0, shuffle=False)\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, num_workers=0, shuffle=False)\n",
    "\n",
    "ucc_data_module = UCC_DataModule(train_path, val_path, attributes)\n",
    "# create datasets\n",
    "ucc_data_module.setup()\n",
    "\n",
    "class UCC_Classifier(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.pretrained_model = AutoModel.from_pretrained(config[\"model_name\"], return_dict=True)\n",
    "        \n",
    "        # appending a classification layer (=\"head\") & a hidden layer before final_layer\n",
    "        self.hidden = nn.Linear(self.pretrained_model.config.hidden_size, self.pretrained_model.config.hidden_size)\n",
    "        self.classifier = nn.Linear(self.pretrained_model.config.hidden_size, self.config[\"n_labels\"])\n",
    "        \n",
    "        # initializing custom layers, optional, would also be done automatically, but better performance if stated explicitly)\n",
    "        torch.nn.init.xavier_uniform_(self.hidden.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.classifier.weight)\n",
    "\n",
    "         # define loss function\n",
    "        self.loss_func = nn.BCEWithLogitsLoss(reduction=\"mean\")\n",
    "        \n",
    "        # dropout layer (ensures no overfitting); randomly turns on/off random nodes for each training loop; so  model is not dependant on specific node\n",
    "        self.dropout = nn.Dropout()\n",
    "\n",
    "\n",
    "    # Forward pass, labels only needed while training, not for prediction, so set to none by default\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        # RoBERTa model output\n",
    "        model_output = self.pretrained_model(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask\n",
    "        )\n",
    "        last_hidden_state = model_output.last_hidden_state\n",
    "\n",
    "        # Take the mean of the last hidden state\n",
    "        pooled_output = torch.mean(last_hidden_state, 1)\n",
    "\n",
    "        # NN-layers\n",
    "        pooled_output = self.hidden(pooled_output)  # pass sentence(or however tokens are pooled) through hidden layer\n",
    "        pooled_output = self.dropout(pooled_output) # pass through dropout layer\n",
    "        pooled_output = F.relu(pooled_output)   # pass through activatioin function( relu; in this case)\n",
    "        logits = self.classifier(pooled_output)     # pass through classification layer\n",
    "\n",
    "        # calculate loss\n",
    "        loss = 0\n",
    "        if labels is not None:\n",
    "            loss = self.loss_func(logits.view(-1, self.config[\"n_labels\"]), labels.view(-1, self.config[\"n_labels\"]))\n",
    "\n",
    "        # logits = fancy name for model output\n",
    "        return loss, logits\n",
    "\n",
    "    def training_step(self, batch, batch_index):\n",
    "        # get & unpack the output dict ( as in.  ucc_ds.__getitem__(0) into self)\n",
    "        loss, logits = self(**batch)\n",
    "        # save training loss\n",
    "        self.log(\"train_loss\", loss, prog_bar=True, logger= True)\n",
    "        return {\"loss\": loss, \"predictions\": logits, \"labels\": batch[\"labels\"]}\n",
    "\n",
    "\n",
    "    def validation_step(self, batch, batch_index):\n",
    "        loss, logits = self(**batch)\n",
    "        self.log(\"validation_loss\", loss, prog_bar=True, logger= True)\n",
    "        return {\"val_loss\": loss, \"predictions\": logits, \"labels\": batch[\"labels\"]}\n",
    "\n",
    "\n",
    "    def prediction_step(self, batch, batch_index):\n",
    "        __, logits = self(**batch)\n",
    "        # prediction:\n",
    "        return logits\n",
    "\n",
    "    # cofigure optimizers\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = AdamW(self.parameters(), lr=self.config[\"lr\"], weight_decay=self.config[\"w_decay\"])\n",
    "        # Scheduler: \n",
    "        total_steps = self.config[\"train_size\"] / self.config[\"bs\"]\n",
    "        # warmup in config is a percentage\n",
    "        warmup_steps = math.floor(total_steps * self.config[\"warmup\"])\n",
    "\n",
    "        scheduler = get_cosine_schedule_with_warmup(optimizer, warmup_steps, total_steps)\n",
    "        return [optimizer], [scheduler] \n",
    "    \n",
    "\n",
    "\n",
    "# Training loop\n",
    "def train(model, ucc_data_module, config):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # Set up optimizer and scheduler\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=config[\"lr\"], weight_decay=config[\"w_decay\"])\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=config[\"n_epochs\"], eta_min=1e-6)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(config[\"n_epochs\"]):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for batch in ucc_data_module.train_dataloader():\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            optimizer.zero_grad()\n",
    "            loss, _ = model(**batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch in ucc_data_module.val_dataloader():\n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                loss, _ = model(**batch)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{config['n_epochs']}], Train Loss: {train_loss/len(ucc_data_module.train_dataloader()):.4f}, Val Loss: {val_loss/len(ucc_data_module.val_dataloader()):.4f}\")\n",
    "        scheduler.step()\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# # Usage\n",
    "config = {\n",
    "    'model_name': \"distilroberta-base\",\n",
    "    \"n_labels\": len(attributes), \n",
    "    \"bs\": 64,              # batch size \n",
    "    \"lr\": 1.5e-6,\n",
    "    \"warmup\": 0.2,\n",
    "    \"train_size\": len(train_data), \n",
    "    \"w_decay\": 0.001,\n",
    "    \"n_epochs\": 1\n",
    "    }\n",
    "\n",
    "train_dataset = UCC_Dataset(train_path, tokenizer, attributes)\n",
    "val_dataset = UCC_Dataset(val_path, tokenizer, attributes, sample=None)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=config['bs'], shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=config['bs'], shuffle=False)\n",
    "\n",
    "model = UCC_Classifier(config)\n",
    "trained_model = train(model, ucc_data_module, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Single Prediction\n",
    "def predict_single(model, ucc_data_module):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Get a random index from the test dataset\n",
    "    random_index = random.randint(0, len(ucc_data_module.val_dataset) - 1)\n",
    "\n",
    "    # Retrieve a sample directly from the dataset\n",
    "    sample = ucc_data_module.val_dataset[random_index]\n",
    "    \n",
    "    # Prepare the sample for input\n",
    "    input_ids = sample['input_ids'].unsqueeze(0).to(device)\n",
    "    attention_mask = sample['attention_mask'].unsqueeze(0).to(device)\n",
    "    labels = sample['labels'].unsqueeze(0).to(device) if 'labels' in sample else None\n",
    "\n",
    "    # Make a prediction for the single sample\n",
    "    with torch.no_grad():\n",
    "        _, logits = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        probabilities = torch.sigmoid(logits).squeeze()\n",
    "\n",
    "    # Return the raw probabilities or binary labels (as tensor)\n",
    "    predicted_labels = (probabilities > 0.5).int()\n",
    "\n",
    "    return probabilities, predicted_labels\n",
    "\n",
    "# Predict a single, random sample from the test set\n",
    "probabilities, predicted_labels = predict_single(trained_model, ucc_data_module)\n",
    "print(f\"Predicted probabilities: {probabilities}\")\n",
    "print(f\"Predicted labels: {predicted_labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict for full test set\n",
    "\n",
    "def predict_full_dataset(model, ucc_data_module):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    all_predicted_labels = []\n",
    "    all_true_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in ucc_data_module.val_dataloader():\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            _, logits = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            \n",
    "            # Apply softmax to logits to get probabilities for each class\n",
    "            probabilities = torch.softmax(logits, dim=1)\n",
    "\n",
    "            # Get the index of the max probability (predicted label)\n",
    "            predicted_labels = torch.argmax(probabilities, dim=1)\n",
    "\n",
    "            all_predicted_labels.append(predicted_labels.cpu())\n",
    "            all_true_labels.append(torch.argmax(labels, dim=1).cpu())  # Assuming true labels are also one-hot encoded\n",
    "\n",
    "    # Concatenate all tensors to get full set results\n",
    "    all_predicted_labels = torch.cat(all_predicted_labels, dim=0)\n",
    "    all_true_labels = torch.cat(all_true_labels, dim=0)\n",
    "\n",
    "    return all_predicted_labels, all_true_labels\n",
    "\n",
    "\n",
    "\n",
    "all_predicted_labels, all_true_labels = predict_full_dataset(trained_model, ucc_data_module)\n",
    "\n",
    "# Example: Print the first 5 predictions and corresponding true labels\n",
    "print(f\"Predicted labels (first 5): \\n{all_predicted_labels[:5]}\")\n",
    "print(f\"True labels (first 5): \\n{all_true_labels[:5]}\")\n",
    "\n",
    "# Convert tensors to numpy arrays for metric calculations\n",
    "y_true = np.array(all_true_labels)\n",
    "y_pred = np.array(all_predicted_labels)\n",
    "\n",
    "# Calculate accuracy, precision, recall, and F1 score\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred, average='macro')\n",
    "precision = precision_score(y_true, y_pred, average='macro')\n",
    "recall = recall_score(y_true, y_pred, average='macro')\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation (TODO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = pd.read_csv(val_path)\n",
    "val_data['unhealthy'] = np.where(val_data['healthy'] == 1, 0, 1)\n",
    "true_labels = np.array(val_data[attributes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "for i, attribute in enumerate(attributes):\n",
    "  fpr, tpr, _ = metrics.roc_curve(\n",
    "      true_labels[:,i].astype(int), predictions[:, i])\n",
    "  auc = metrics.roc_auc_score(\n",
    "      true_labels[:,i].astype(int), predictions[:, i])\n",
    "  plt.plot(fpr, tpr, label='%s %g' % (attribute, auc))\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('RoBERTa Trained on UCC Datatset - AUC ROC')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
