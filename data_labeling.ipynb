{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Aug  6 13:31:16 2024    stats.prof\n",
      "\n",
      "         22201 function calls (22186 primitive calls) in 296.596 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "   List reduced from 331 to 10 due to restriction <10>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        1    0.000    0.000  296.596  296.596 {built-in method builtins.exec}\n",
      "        1    0.029    0.029  296.596  296.596 <string>:1(<module>)\n",
      "        1    0.000    0.000  296.568  296.568 C:\\Users\\q524745\\AppData\\Local\\Temp\\ipykernel_1156\\434007758.py:136(main)\n",
      "        1    0.081    0.081  287.275  287.275 C:\\Users\\q524745\\AppData\\Local\\Temp\\ipykernel_1156\\434007758.py:82(process_log_files)\n",
      "       32    0.006    0.000  287.193    8.975 C:\\Users\\q524745\\AppData\\Local\\Temp\\ipykernel_1156\\434007758.py:53(process_single_log_file)\n",
      "       56    0.000    0.000  284.910    5.088 C:\\Users\\q524745\\AppData\\Local\\Temp\\ipykernel_1156\\434007758.py:42(check_log_entry)\n",
      "       56  284.889    5.087  284.909    5.088 C:\\Users\\q524745\\AppData\\Local\\Temp\\ipykernel_1156\\434007758.py:47(<listcomp>)\n",
      "        1    0.026    0.026    9.287    9.287 C:\\Users\\q524745\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\core\\generic.py:3297(to_csv)\n",
      "        1    0.000    0.000    9.261    9.261 C:\\Users\\q524745\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\io\\formats\\format.py:1056(to_csv)\n",
      "        1    0.000    0.000    9.261    9.261 C:\\Users\\q524745\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\io\\formats\\csvs.py:232(save)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pstats.Stats at 0x26e4ce2f708>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import os\n",
    "import textwrap\n",
    "import extract_build_failures.error_patterns as error_patterns\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import cProfile\n",
    "\n",
    "INFRA_PATTERNS = error_patterns.INFRA_PATTERNS\n",
    "BUILD_PATTERNS = error_patterns.BUILD_PATTERNS\n",
    "\n",
    "def restructure_patterns(patterns):\n",
    "    \"\"\"\n",
    "    Restructures the input patterns dictionary to a more structured format.\n",
    "    \"\"\"\n",
    "    restructured_patterns = {}\n",
    "    for pattern_type, subpatterns in patterns.items():\n",
    "        if isinstance(subpatterns, (set, list)):\n",
    "            restructured_patterns[pattern_type] = {\"\": list(subpatterns)}\n",
    "        elif isinstance(subpatterns, dict):\n",
    "            restructured_patterns[pattern_type] = {}\n",
    "            for subtype, regex_list in subpatterns.items():\n",
    "                if isinstance(regex_list, (list, set)):\n",
    "                    restructured_patterns[pattern_type][subtype] = list(regex_list)\n",
    "                else:\n",
    "                    raise ValueError(f\"Unexpected type for regex list: {type(regex_list)}\")\n",
    "        else:\n",
    "            raise ValueError(f\"Unexpected type for subpatterns: {type(subpatterns)}\")\n",
    "    return restructured_patterns\n",
    "\n",
    "def compile_patterns(patterns_dict):\n",
    "    \"\"\"\n",
    "    Compiles the restructured patterns dictionary into a dictionary of compiled regex patterns.\n",
    "    \"\"\"\n",
    "    compiled_patterns = {}\n",
    "    for main_category, subpatterns in patterns_dict.items():\n",
    "        for sub_category, patterns in subpatterns.items():\n",
    "            compiled_patterns.setdefault((main_category, sub_category), []).extend([re.compile(pattern) for pattern in patterns])\n",
    "    return compiled_patterns\n",
    "\n",
    "def check_log_entry(log_entry, compiled_patterns):\n",
    "    \"\"\"\n",
    "    Checks the given log entry against the compiled patterns and returns the matches.\n",
    "    \"\"\"\n",
    "    return [\n",
    "        (main_category, sub_category or main_category, regex.pattern)\n",
    "        for (main_category, sub_category), regex_list in compiled_patterns.items()\n",
    "        for regex in regex_list\n",
    "        for _ in regex.finditer(log_entry)\n",
    "    ]\n",
    "\n",
    "def process_single_log_file(file_path, compiled_patterns):\n",
    "    \"\"\"\n",
    "    Processes a single log file and returns the summary, task matches, and dataset.\n",
    "    \"\"\"\n",
    "    summary = {'tasks_summary': {}}\n",
    "    task_matches = {}\n",
    "    dataset = []\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        log_entries = json.load(file)\n",
    "        for log in log_entries:\n",
    "            task_id = log.get('id', 'No ID provided')\n",
    "            name = log.get('name', 'No task name provided')\n",
    "            task_key = f\"{name} (ID: {task_id})\"\n",
    "            summary['tasks_summary'].setdefault(task_key, 0)\n",
    "            summary['tasks_summary'][task_key] += 1\n",
    "\n",
    "            stdout_text = \"\\n\".join(log.get('stdout_lines', []))\n",
    "            matches = check_log_entry(stdout_text, compiled_patterns)\n",
    "            if matches:\n",
    "                task_matches.setdefault(task_key, defaultdict(set))\n",
    "                for match in matches:\n",
    "                    error_cluster, error_type, pattern = match\n",
    "                    task_matches[task_key][(error_cluster, error_type, pattern)].add(stdout_text)\n",
    "                for (error_cluster, error_type, pattern), log_entries in task_matches[task_key].items():\n",
    "                    dataset.append((task_id, \"\\n\".join(log_entries), error_cluster, error_type))\n",
    "\n",
    "    return summary, task_matches, dataset\n",
    "\n",
    "def process_log_files(directory_path, compiled_patterns):\n",
    "    \"\"\"\n",
    "    Processes all log files in the given directory and returns the summary, file matches, and dataset.\n",
    "    \"\"\"\n",
    "    final_summary = {'tasks_summary': {}}\n",
    "    file_matches = {}\n",
    "    all_dataset = []\n",
    "\n",
    "    for filename in filter(lambda f: f.endswith('.json'), os.listdir(directory_path)):\n",
    "        file_path = os.path.join(directory_path, filename)\n",
    "        summary, task_matches, dataset = process_single_log_file(file_path, compiled_patterns)\n",
    "\n",
    "        for task_key, count in summary['tasks_summary'].items():\n",
    "            final_summary['tasks_summary'].setdefault(task_key, 0)\n",
    "            final_summary['tasks_summary'][task_key] += count\n",
    "\n",
    "        if task_matches:\n",
    "            file_matches[file_path] = task_matches\n",
    "            \n",
    "        all_dataset.extend(dataset)\n",
    "\n",
    "    return all_dataset\n",
    "\n",
    "def format_summary_to_screen_width(summary, terminal_width=150):\n",
    "    \"\"\"\n",
    "    Formats the summary dictionary to fit the given terminal width.\n",
    "    \"\"\"\n",
    "    formatted_summary = \"\"\n",
    "    for key, value in summary.items():\n",
    "        if isinstance(value, dict):\n",
    "            formatted_summary += f\"{key}:\\n\"\n",
    "            for sub_key, sub_value in value.items():\n",
    "                wrapped_sub_value = textwrap.fill(str(sub_value), terminal_width - 4)\n",
    "                formatted_summary += f\"  {sub_key}: {wrapped_sub_value}\\n\"\n",
    "        else:\n",
    "            wrapped_value = textwrap.fill(str(value), terminal_width)\n",
    "            formatted_summary += f\"{key}: {wrapped_value}\\n\"\n",
    "        formatted_summary += \"-\" * terminal_width + \"\\n\"\n",
    "    return formatted_summary\n",
    "\n",
    "def format_file_matches(file_matches, terminal_width=150):\n",
    "    \"\"\"\n",
    "    Formats the file matches dictionary to fit the given terminal width.\n",
    "    \"\"\"\n",
    "    formatted_matches = \"\"\n",
    "    for file_path, tasks in file_matches.items():\n",
    "        formatted_matches += f\"File: {file_path}\\n\"\n",
    "        for task_key, matches in tasks.items():\n",
    "            formatted_matches += f\"  Task: {task_key}\\n\"\n",
    "            for (error_cluster, error_type, pattern), log_entries in matches.items():\n",
    "                formatted_matches += f\"    Error Cluster: {error_cluster}, Error Type: {error_type or error_cluster}, Pattern: {pattern}\\n\"\n",
    "        formatted_matches += \"-\" * terminal_width + \"\\n\"\n",
    "    return formatted_matches\n",
    "\n",
    "def main():\n",
    "    # Restructure the patterns first\n",
    "    restructured_infra_patterns = restructure_patterns(INFRA_PATTERNS)\n",
    "    restructured_build_patterns = restructure_patterns(BUILD_PATTERNS)\n",
    "\n",
    "    # Compile the restructured patterns\n",
    "    compiled_infra_patterns = compile_patterns(restructured_infra_patterns)\n",
    "    compiled_build_patterns = compile_patterns(restructured_build_patterns)\n",
    "\n",
    "    # Combine all compiled patterns\n",
    "    all_compiled_patterns = {**compiled_infra_patterns, **compiled_build_patterns}\n",
    "\n",
    "    # Process log files\n",
    "    directory_path = 'preprocessed_logs'\n",
    "    dataset = process_log_files(directory_path, all_compiled_patterns)\n",
    "\n",
    "    # Save the dataset\n",
    "    dataset_df = pd.DataFrame(dataset, columns=['task_id', 'log_entry', 'error_cluster', 'error_type'])\n",
    "    dataset_df.to_csv('labeled_dataset.csv', index=True)\n",
    "    # print(dataset)\n",
    "    return dataset_df\n",
    "\n",
    "# dataset = main()\n",
    "\n",
    "cProfile.run('main()', 'stats.prof')\n",
    "\n",
    "import pstats\n",
    "p = pstats.Stats('stats.prof')\n",
    "p.sort_stats('cumulative').print_stats(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
