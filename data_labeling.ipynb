{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### not needed anymore? \n",
    "import error_patterns\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "# INFRA_PATTERNS = {\n",
    "#     \"Infrastructure - Docker\": [\n",
    "#         r\"=+\\n.+\\n=+\\n+Cannot connect to the Docker daemon .+\",\n",
    "#         r\"=+\\n.+\\n=+\\n+Error response .+\",\n",
    "#         r\"^.*Error: unable to pull ddad.artifactory.cc.bmwgroup.net.*\",\n",
    "#         r\"Error\\: error logging into \\\"ddad\\.artifactory\\.cc\\.bmwgroup\\.net\\\"\\: invalid username\\/password\",\n",
    "#         r\"Error response from daemon(.*)artifactory\\.cc\\.bmwgroup\\.net[^\\s]*\\s(?P<details>.*)\",\n",
    "#         r\"manifest for artifactory\\.cc\\.bmwgroup\\.net\\/.*\\:latest not found\\: manifest unknown\\: The named manifest is not known to the registry\",\n",
    "#         r\"^Error: Error parsing image configuration: unable to retrieve auth token: invalid username/password: unknown: Bad props auth token(:?).*$\",\n",
    "#     ]\n",
    "# }\n",
    "\n",
    "\n",
    "# Define the regex patterns as provided\n",
    "BUILD_PATTERNS = error_patterns.BUILD_PATTERNS\n",
    "INFRA_PATTERNS = error_patterns.INFRA_PATTERNS\n",
    "\n",
    "def compile_patterns(patterns_dict):\n",
    "    compiled_patterns = {}\n",
    "    for tool, patterns in patterns_dict.items():\n",
    "        # If the value is a set, convert it to a list before compiling\n",
    "        if isinstance(patterns, set):\n",
    "            patterns = list(patterns)\n",
    "        \n",
    "        # If the value is a list, compile the patterns directly\n",
    "        if isinstance(patterns, list):\n",
    "            compiled_patterns[tool] = [re.compile(p) for p in patterns]\n",
    "        # If the value is another dictionary, compile patterns in each sub-dictionary\n",
    "        elif isinstance(patterns, dict):\n",
    "            compiled_patterns[tool] = {}\n",
    "            for error_type, sub_patterns in patterns.items():\n",
    "                # If the sub_patterns is a set, convert it to a list\n",
    "                if isinstance(sub_patterns, set):\n",
    "                    sub_patterns = list(sub_patterns)\n",
    "                if isinstance(sub_patterns, list):\n",
    "                    compiled_patterns[tool][error_type] = [re.compile(p) for p in sub_patterns]\n",
    "                else:\n",
    "                    raise TypeError(f\"Expected a list for error type '{error_type}' in tool '{tool}', got {type(sub_patterns)}\")\n",
    "        else:\n",
    "            raise TypeError(f\"Expected a list, set, or a dict for tool '{tool}', got {type(patterns)}\")\n",
    "    return compiled_patterns\n",
    "\n",
    "# Compile the regex patterns for efficiency\n",
    "#compiled_build_patterns = compile_patterns(BUILD_PATTERNS)\n",
    "# compiled_infra_patterns = compile_patterns(INFRA_PATTERNS)\n",
    "\n",
    "# compiled_patterns = {\n",
    "#     \"BUILD\": compiled_build_patterns,\n",
    "#     \"INFRA\": compiled_infra_patterns,\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2aa698b8-b2fd-a900-8da6-00000000002d docker-run  non-zero return code\n",
      "The label for the first log entry is: Unknown\n"
     ]
    }
   ],
   "source": [
    "#### use example pattern\n",
    "\n",
    "import os\n",
    "import json\n",
    "from collections import OrderedDict, defaultdict\n",
    "import error_patterns\n",
    "import re\n",
    "\n",
    "\n",
    "\n",
    "INFRA_PATTERNS = error_patterns.INFRA_PATTERNS\n",
    "\n",
    "def read_cropped_logs(directory_path):\n",
    "    \"\"\"\n",
    "    Read the cropped files and store them in a dictionary with ordered keys.\n",
    "\n",
    "    :param directory_path: The directory path to the JSON data files.\n",
    "    :return: A dictionary with filenames as keys and their corresponding JSON content as values.\n",
    "    \"\"\"\n",
    "    data = {}\n",
    "    key_order = ['id', 'name', 'stderr', 'stdout_lines', 'node', 'msg']\n",
    "    # Loop through all files in the directory\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith('.json'):\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "\n",
    "            with open(file_path, 'r') as file:\n",
    "                file_content = json.load(file)\n",
    "                # Create a new ordered dictionary for each JSON object\n",
    "                ordered_file_content = [\n",
    "                    OrderedDict((k, content.get(k, None)) for k in key_order)\n",
    "                    for content in file_content\n",
    "                ]\n",
    "                # Use the filename as the key for the outer dictionary\n",
    "                data[filename] = {index: content for index, content in enumerate(ordered_file_content)}\n",
    "\n",
    "    return data\n",
    "\n",
    "# Function to compile the patterns\n",
    "def compile_patterns(patterns_dict):\n",
    "    compiled_patterns = defaultdict(list)\n",
    "    for category, patterns in patterns_dict.items():\n",
    "        for pattern in patterns:\n",
    "            compiled_patterns[category].append(re.compile(pattern))\n",
    "    return compiled_patterns\n",
    "\n",
    "# Function to label a single log entry\n",
    "def label_log_entry(log_entry, regex_list):\n",
    "    for regex in regex_list:\n",
    "        if 'msg' in log_entry and regex.search(log_entry['msg']):\n",
    "            return 'Infrastructure - Docker Build Failure'\n",
    "    print(log_entry[\"id\"], \n",
    "          log_entry[\"name\"],\n",
    "          log_entry[\"stderr\"],\n",
    "          log_entry[\"msg\"]\n",
    "         )\n",
    "    return 'Unknown'\n",
    "\n",
    "# Compile the patterns\n",
    "compiled_infra_patterns = compile_patterns(INFRA_PATTERNS)\n",
    "\n",
    "#USAGE: \n",
    "cropped_logs_path = 'preprocessed_logs'\n",
    "nested_data = read_cropped_logs(cropped_logs_path)\n",
    "\n",
    "# Extract the first log entry from the nested data\n",
    "first_file_key = next(iter(nested_data))\n",
    "first_log_entry = nested_data[first_file_key][0]\n",
    "\n",
    "# Label the first log entry\n",
    "# Pass the list of compiled Infrastructure - Docker patterns directly to the label_log_entry function\n",
    "label = label_log_entry(first_log_entry, compiled_infra_patterns['Infrastructure - Docker'])\n",
    "print(f\"The label for the first log entry is: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### compile known pattern\n",
    "\n",
    "\n",
    "import error_patterns\n",
    "import json\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "# Define the regex patterns as provided\n",
    "BUILD_PATTERNS = error_patterns.BUILD_PATTERNS\n",
    "INFRA_PATTERNS = error_patterns.INFRA_PATTERNS\n",
    "\n",
    "def compile_patterns(patterns_dict):\n",
    "    compiled_patterns = {}\n",
    "    for tool, patterns in patterns_dict.items():\n",
    "        # If the value is a set, convert it to a list before compiling\n",
    "        if isinstance(patterns, set):\n",
    "            patterns = list(patterns)\n",
    "        \n",
    "        # If the value is a list, compile the patterns directly\n",
    "        if isinstance(patterns, list):\n",
    "            compiled_patterns[tool] = [re.compile(p) for p in patterns]\n",
    "        # If the value is another dictionary, compile patterns in each sub-dictionary\n",
    "        elif isinstance(patterns, dict):\n",
    "            compiled_patterns[tool] = {}\n",
    "            for error_type, sub_patterns in patterns.items():\n",
    "                # If the sub_patterns is a set, convert it to a list\n",
    "                if isinstance(sub_patterns, set):\n",
    "                    sub_patterns = list(sub_patterns)\n",
    "                if isinstance(sub_patterns, list):\n",
    "                    compiled_patterns[tool][error_type] = [re.compile(p) for p in sub_patterns]\n",
    "                else:\n",
    "                    raise TypeError(f\"Expected a list for error type '{error_type}' in tool '{tool}', got {type(sub_patterns)}\")\n",
    "        else:\n",
    "            raise TypeError(f\"Expected a list, set, or a dict for tool '{tool}', got {type(patterns)}\")\n",
    "    return compiled_patterns\n",
    "\n",
    "# Compile the regex patterns for efficiency\n",
    "compiled_build_patterns = compile_patterns(BUILD_PATTERNS)\n",
    "compiled_infra_patterns = compile_patterns(INFRA_PATTERNS)\n",
    "\n",
    "compiled_patterns = {\n",
    "    \"BUILD\": compiled_build_patterns,\n",
    "    \"INFRA\": compiled_infra_patterns,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## use imported patterns\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "import json\n",
    "from collections import OrderedDict\n",
    "\n",
    "def read_cropped_logs(directory_path):\n",
    "    \"\"\"\n",
    "    Read the cropped files and store them in a dictionary with ordered keys.\n",
    "\n",
    "    :param directory_path: The directory path to the JSON data files.\n",
    "    :return: A dictionary with filenames as keys and their corresponding JSON content as values.\n",
    "    \"\"\"\n",
    "    data = {}\n",
    "    key_order = ['id', 'name', 'stderr', 'stdout_lines', 'node', 'msg']\n",
    "    # Loop through all files in the directory\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith('.json'):\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "\n",
    "            with open(file_path, 'r') as file:\n",
    "                file_content = json.load(file)\n",
    "                # Create a new ordered dictionary for each JSON object\n",
    "                ordered_file_content = [\n",
    "                    OrderedDict((k, content.get(k, None)) for k in key_order)\n",
    "                    for content in file_content\n",
    "                ]\n",
    "                # Use the filename as the key for the outer dictionary\n",
    "                data[filename] = {index: content for index, content in enumerate(ordered_file_content)}\n",
    "\n",
    "    return data\n",
    "\n",
    "def label_logs(nested_logs, compiled_patterns):\n",
    "    labeled_logs = {}\n",
    "    for filename, file_content in nested_logs.items():\n",
    "        labeled_file_logs = []\n",
    "        for index, log_entry in file_content.items():\n",
    "            labels = defaultdict(list)\n",
    "            # Check each category of patterns\n",
    "            for category, pattern_dict in compiled_patterns.items():\n",
    "                for tool, regex_list in pattern_dict.items():\n",
    "                    for regex in regex_list:\n",
    "                        # Assuming 'message' is the field containing the text\n",
    "                        if 'message' in log_entry and regex.search(log_entry['message']):\n",
    "                            labels[category].append(tool)\n",
    "                            break  # Stop after the first match within the same tool\n",
    "            # Append labels to log entry\n",
    "            log_entry['labels'] = dict(labels)\n",
    "            labeled_file_logs.append(log_entry)\n",
    "        # Store the labeled logs using the filename as the key\n",
    "        labeled_logs[filename] = labeled_file_logs\n",
    "    return labeled_logs\n",
    "\n",
    "# Usage:\n",
    "cropped_logs_path = 'preprocessed_logs'\n",
    "nested_data = read_cropped_logs(cropped_logs_path)\n",
    "\n",
    "# Label logs\n",
    "labeled_nested_logs = label_logs(nested_data, compiled_patterns)\n",
    "\n",
    "# print the labeled logs for the first file in the dataset\n",
    "first_file_key = next(iter(labeled_nested_logs))\n",
    "print(json.dumps(labeled_nested_logs[first_file_key], indent=4))\n",
    "\n",
    "# Usage: \n",
    "cropped_logs_path = 'preprocessed_logs'\n",
    "data = read_cropped_logs(cropped_logs_path)\n",
    "\n",
    "# Label  logs\n",
    "# labeled_logs = label_logs(data, compiled_patterns)\n",
    "# print(labeled_logs[0])\n",
    "# # Output the labeled data\n",
    "# with open('labeled_logs.json', 'w') as file:\n",
    "#     json.dump(labeled_logs, file, indent=4)\n",
    "\n",
    "# print(\"Labeling complete. Labeled logs saved to 'labeled_logs.json'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
